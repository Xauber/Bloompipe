# ===== Inits + Definitions =========================
import math
import sys
import os
import numpy as np
import torch
import random

from matplotlib import pyplot as plt
from tqdm import tqdm
from scipy.stats import truncnorm
from PIL import Image
from .ArrayInterpolations import *

LSD_DIRECTORY = str(os.getcwd())
sys.path.insert(0, LSD_DIRECTORY + "/Synthesis")
from stylegan2 import dnnlib, legacy


# ===== Global-Variables =========================

# This enables benchmark mode in cudnn.
# Cudnn will look for the optimal set of algorithms for our input shape
torch.backends.cudnn.benchmark = True

# Setting random seeds to start from same frame every time
np.random.seed(3111)
# random.seed(1222)
PLOT_OUTPUT_PATH = "../../Bloompipe_Test/plot_sequence/vector_movement_{}.png"


# ===== Methods ==================================
class BloomyDreams:
    """

    BloomyDreams is the most important module that initializes the StyleGAN weights,
    generates input vectors for the GAN synchronized to the audio,
    and finally generates the frames by feeding the vectors
    into the GAN and storing the output images in a folder.
    This module is based on the open source LucidSonicDreams Code:
    https://github.com/mikaelalafriz/lucid-sonic-dreams

    We restructured the code completely and refactored it. Furthermore,
    we added song Sections in order to separate different
    sections of the song from each other to generate a bigger pool of visual content
    for coherent parts of the song.
    We also added the feature of looping the video, we added different types of interpolations
    between the initial separate vectors, and we changed the generation of the vector path into  2 different steps.

    The base motion is the general vector path through the latent space, which can be
    generated by using generatePointVectorBase, which randomizes a user selected amount of random vectors
    and interpolates between them. If visualizeSections is active this base motion is created differently
    and creates a different vector for each sectionStart and only interpolates within the section.
    Onto this base motion we combine it with an additional motion by generateMotionVectors.
    This motion is responsible for syncing the final images to the audio by varying from the base path
    dependent on the audio data. For addressing the StyleGAN in styleganInit we used the code of LucidSonicDreams
    and added GPU support.

    """

    def __init__(
            self,
            pulseAudio,
            songSections,
            showPlots: bool = False,
            fps: int = 25,
            style: str = "obama"
    ):
        """
        The constructor of the BloomyDreams class initializes all class variables
        and sets some default values for these and also the amount of frames/interpolations
        Args:
            pulseAudio: The audio data as a float array
            songSections: Section beginnings in seconds as a float array
            showPlots: Boolean if the vector motion should be plotted
            fps: Frames per second of the output video
            style: The .pkl file that should be used
        """
        print("Creating the Dream object...")

        # Defining class attributes
        self.videoSmoothness = None  # How smoothly the video reacts to the audio
        self.pulseAudio = pulseAudio  # Audio sequence to which the video reacts
        self.songSections = songSections  # Beginnings of song sections in seconds
        self.sectionSimilarity = 1.0  # Internal visual similarity of sections
        self.visualizeSections = False  # If audio sections should be visualized differently
        self.loopVideo = None  # If the video should loop
        self.showPlots = showPlots  # If vector movement is plotted as a graph
        self.Gs = None  # Stylegan model weights
        self.truncation = 1.0  # Limitation of the latent space
        self.pulseReact = None  # How much the video should react to the pulse
        self.randomPointAmount = None  # How many random latent space vectors are generated
        self.interpolationType = None  # What kind of interpolation is applied between the different points
        self.motionRandomness = None  # How much the motion between the vectors jiggles
        self.fps = fps  # FPS of the generated video
        self.outPath = None  # Path of the output file
        self.pulseDirection = None  # Motion direction of vector dimensions
        self.currentMotion = None  # Motion vector of current frame used to update motion directions
        self.frameDuration = None  # Frame duration in seconds
        self.numFrames = len(pulseAudio)  # Amount of frames
        self.finalMotion = None  # Motion vectors of the generate function
        self.style = style  # Picture style
        self.numDimensions = 512  # Amount of vector dimensions
        self.styleExists = False  # Checks if style has already been loaded

        print("Frames in current video: {}".format(self.numFrames))

    def hallucinate(
            self,
            outPath: str,
            loopVideo: bool = False,
            visualizeSections: bool = False,
            sectionSimilarity: float = 1.0,
            pointAmount: int = 5,
            interpolationType: str = "cubic",
            pulseReact: float = 0.5,
            videoSmoothness: float = 0.75,
            motionRandomness: float = 0.5,
            truncation: float = 1.0
    ):
        """
        This is the full pipeline of the video generation.
        Here, we do the type checking of the input parameters,
        Start the style loading, the vector generation and the image synthesis.
        Args:
            outPath:
            loopVideo:
            visualizeSections:
            sectionSimilarity:
            pointAmount:
            interpolationType:
            pulseReact:
            videoSmoothness:
            motionRandomness:
            truncation:

        Returns: Status of the synthesis success

        """
        print("Starting image generation...")

        try:
            # Check if directory exists and abort if not
            if not (os.path.exists(outPath)):
                return "directoryNotFound"

            # Raise exception if point amount is bigger than
            if pointAmount > self.numFrames:
                sys.exit('pointAmount must not be greater than the amount of frames')

            # Raise exception if any of the following parameters are not between 0 and 1
            for param in ["motionRandomness", "truncation", "sectionSimilarity"]:
                if (locals()[param]) and not (0 <= locals()[param] <= 1):
                    sys.exit("{} must be between 0 and 1".format(param))

            if videoSmoothness > 0.9:
                sys.exit("{} must be between 0.0 and 0.9".format(videoSmoothness))

            # Raise exception if point amount is 0
            if pointAmount < 1:
                sys.exit("pointAmount must be at least 1")

            self.outPath = outPath
            self.loopVideo = loopVideo
            self.visualizeSections = visualizeSections
            self.sectionSimilarity = sectionSimilarity
            self.randomPointAmount = pointAmount
            self.interpolationType = interpolationType
            self.pulseReact = pulseReact
            self.videoSmoothness = videoSmoothness
            self.motionRandomness = motionRandomness
            self.truncation = truncation

            # Initialise style
            if not self.styleExists:
                if not callable(self.style):
                    self.styleganInit()

                self.styleExists = True

            # Generate vectors
            self.generateMotionVectors()

            # Generate frames
            self.generateFrames(outPath)

            return "synthesisSuccessful"
        except Exception as exception:
            print(exception)
            return "synthesisFailed"

    def styleganInit(self):
        """
        Initializes the stylegan weights by loading the .pkl file onto the cuda/cpu device
        Returns: -

        """
        print("Preparing style...")

        style = self.style

        # If style is not a .pkl file path add .pkl ending
        if '.pkl' not in style:
            weightsFile = style + '.pkl'
        # Set the weights file to the provided filename
        else:
            weightsFile = style

        # Load generator
        print(f'Loading weights from {weightsFile}...')

        # Set device to CUDA GPU or CPU
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Open the weights and load them onto the set device
        with dnnlib.util.open_url(weightsFile) as pkl_file:
            self.Gs = legacy.load_network_pkl(pkl_file)['G_ema'].to(device)  # type: ignore

    def generateMotionVectors(self):
        """
        Generates motion vectors as inputs for each frame.
        This generates a vector base with a vector for each frame.
        Onto each vector, it adds the loudness of the corresponding sample.
        Returns: -

        """
        print("Generating vectors...")

        # Generate vector base interpolation depending on whether
        # we want to visualize the song sections or move between
        # evenly spaced random points in time
        if self.visualizeSections:
            baseMotion = self.generateSectionVectorBase()
        else:
            baseMotion = self.generatePointVectorBase()

        print("Frames in base motion: {}".format(len(baseMotion)))

        # Initialise lists of added motion vectors
        addedMotions = []
        randomDirectionFactors = []

        # Initialise pulse factor vectors based on Pulse Reactivity value
        pulseFactor = np.array([self.pulseReact] * self.numDimensions)

        # Randomly initialise directions of the added motion vectors
        self.pulseDirection = np.array([random.choice([1, -1]) for _ in range(self.numDimensions)])

        # Initialize motion based on frame amount and dimension count
        finalMotion = np.empty([self.numFrames, self.numDimensions])

        # Updating final motion vectors to synchronize with the audio
        # Add corresponding loudness to each base vector
        print("Syncing base motion to audio...")

        for f in range(self.numFrames):
            randomDirectionFactor = np.array([(random.uniform(-1, 1) * self.motionRandomness
                                               + 1 * (1 - self.motionRandomness))
                                              for _ in range(self.numDimensions)])

            # Normalize direction factors to always be of length 1
            randomDirectionFactor = randomDirectionFactor / np.linalg.norm(randomDirectionFactor)

            # Smooth each random direction vector using a weighted average of
            # itself and the previous vector
            if f > 0:
                randomDirectionFactor = randomDirectionFactors[f - 1] * self.videoSmoothness\
                                        + randomDirectionFactor * (1 - self.videoSmoothness)

            # Append random direction factors to list
            randomDirectionFactors.append(randomDirectionFactor)

            # Generate incremental update vectors for Pulse
            currentPulse = pulseFactor * float(self.pulseAudio[f])

            # Update directions
            self.pulseDirection = self.updatePulseDirections(baseMotion[f], currentPulse[0])

            # Combine pulse and direction
            addedMotion = currentPulse * self.pulseDirection
            addedMotionLength = np.linalg.norm(addedMotion)

            # Rotate pulse direction randomly
            addedMotion = addedMotion * randomDirectionFactor
            if addedMotionLength != 0:
                addedMotion = addedMotion / np.linalg.norm(addedMotion) * addedMotionLength

            # Smooth each update vector using a weighted average of
            # itself and the previous vector
            if f > 0:
                addedMotion = addedMotions[f - 1] * self.videoSmoothness + addedMotion * (1 - self.videoSmoothness)

            # Append added motion vectors to list
            addedMotions.append(addedMotion)

            # Update final motion vectors by adding the base motion and the added motion
            finalMotion[f] = baseMotion[f] + addedMotion

        self.finalMotion = finalMotion

        if self.showPlots:
            self.plotDimensions(savePlots=False, plotFullPath=True)

    def generateSectionVectorBase(self):
        """
        Generating the vector base, based on the section beginnings.
        For each section it generates a start and an end vector.
        The higher the section similarity, the closer those vectors are to each other.
        Returns: -

        """
        print("Generating base motion for sections...")

        # Initialize vector base
        baseMotion = np.array([0.0] * 0).reshape(0, 512)
        numSections = len(self.songSections)

        # Storage for the beginning of the video for looping
        firstSectionStart = []

        # For each section generate an interpolation between two random vectors
        for i in range(numSections):
            # Calculate the amount of frames in the current section
            sectionStart = self.songSections[i]

            # For the last section, only generate as many frames as provided with the audio data
            if i == len(self.songSections) - 1:
                numFramesInSection = self.numFrames - len(baseMotion)
            else:
                # Calculating length of each section
                sectionEnd = self.songSections[i + 1]
                sectionLength = round(sectionEnd - sectionStart, 12)

                numFramesInSection = math.floor(sectionLength * self.fps)

            # Only generate vectors for section if section has frames (is longer than 1 / fps seconds)
            if numFramesInSection > 0:
                # Generate vectors for start and end of section
                randomPointStart = self.truncation * truncnorm.rvs(-2, 2, size=(1, self.numDimensions)).astype(np.float32)[
                    0]

                if self.loopVideo:
                    # Store the first vector for looping
                    if i == 0:
                        firstSectionStart = randomPointStart
                    # Depending on section similarity, bring the start vector of the last section
                    # closer to the end vector
                    elif i == numSections - 1:
                        randomPointStart = firstSectionStart * self.sectionSimilarity \
                                           + randomPointStart * (1 - self.sectionSimilarity)

                # Depending on section similarity, bring the end vector closer to the start vector
                randomPointEnd = self.truncation * truncnorm.rvs(-2, 2, size=(1, self.numDimensions)).astype(np.float32)[0]
                randomPointEnd = randomPointStart * self.sectionSimilarity \
                                 + randomPointEnd * (1 - self.sectionSimilarity)

                # Set last vector to first vector if the video should loop
                if self.loopVideo:
                    if i == numSections - 1:
                        randomPointEnd = firstSectionStart

                # Add section interpolation to motion vector array
                sectionMotion = interpolateVectors((randomPointStart, randomPointEnd), numFramesInSection,
                                                   self.interpolationType)
                baseMotion = np.append(baseMotion, sectionMotion, axis=0)

        return baseMotion

    def generatePointVectorBase(self):
        """
        Generating the vector base, based on a given amount of points in space
        Returns:
            baseMotion: the basic interpolation between the random points in latent space

        """
        print("Generating base motion for random points...")

        # Get number of random vectors to initialise (based on random_point_amount)
        numRandomPoints = self.randomPointAmount

        # Truncation limits the min/max values in the random vector
        # Initialise vectors
        randomPoints = [self.truncation * truncnorm.rvs(-2, 2, size=(1, self.numDimensions)).astype(np.float32)[0]
                        for _ in range(numRandomPoints)]
        # Use a gaussian distribution of the dimension values
        # randomPoints = [np.array([random.gauss(0, 0.5) for _ in range(self.input_shape)], dtype=float)
        #                 for _ in range(numRandomPoints)]

        # Set last vector to first vector if the video should loop
        if self.loopVideo:
            randomPoints[-1] = randomPoints[0]

        # Interpolate between random vectors
        baseMotion = interpolateVectors(randomPoints, self.numFrames, self.interpolationType)

        return baseMotion

    def updatePulseDirections(self, currentMotion, currentPulse):
        """
        Limit pulse strength to truncation level to avoid moving outside the defined space.
        Update direction of pulse motion based on truncation value.
        Args:
            currentMotion: The current position of the motion inside the latent space
            currentPulse: The amplitude of the vector movement at the current frame

        Returns: Updated pulse directions that direct each dimension away from the space boundary

        """
        p = currentPulse
        t = self.truncation  # How much the space is limited
        pulseDirections = self.pulseDirection  # In what direction each dimension is currently moving

        # For each current value in the motion vector, change direction if absolute
        # value +/- currentPulse is larger than 2 * truncation (i.e. the space boundaries)
        update = lambda cm, pd: 1 if cm - p < -2 * t else \
            -1 if cm + p >= 2 * t else pd

        updateVector = np.vectorize(update)

        return updateVector(currentMotion, pulseDirections)

    def plotDimensions(self, savePlots, plotFullPath):
        """
        Plotting the first 2 dimensions of the vector motion
        Args:
            savePlots: If the plot pictures should be saved to the disk
            plotFullPath: If the line should be plotted as a whole, or each point on its own

        Returns:

        """
        # Plot point for each interpolation/frame
        for f in range(self.numFrames):
            x = self.finalMotion[f][0]  # Dimension 1
            y = self.finalMotion[f][1]  # Dimension 2

            # Clear plot to show only individual points
            if not plotFullPath:
                plt.clf()

            # Limit plane to -2, 2
            plt.xlim(-2, 2)
            plt.ylim(-2, 2)

            # Plot current point
            plt.scatter(x, y)

            # Save current plot as frame
            if savePlots:
                plt.savefig(PLOT_OUTPUT_PATH.format(f))

        plt.show()

    def generateFrames(self, path):
        """
        Generates GAN output for each frame of the video
        Feeds the vector motion to the network frame by frame
        and saves the generated images
        Args:
            path: Path to the output folder of the frames

        Returns: -

        """
        print("Hallucinating (Generating frames)...")

        # Setting synthesis arguments for the generator
        GsSynKWArgs = {'noise_mode': 'const'}  # random, const, None

        # Sets the device to generate on
        # If cuda kernels are available use gpu, otherwise use cpu
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Define vector-batch size and amount of batches
        batchSize = 14
        numBatches = math.ceil(len(self.finalMotion)/batchSize)

        # Generate all frames
        for i in tqdm(range(numBatches), position=0, leave=True):
            # Obtain current motion vector batch
            if i == numBatches - 1:
                motionBatch = np.array(self.finalMotion[i * batchSize:len(self.finalMotion)])
            else:
                motionBatch = np.array(self.finalMotion[i * batchSize:(i + 1) * batchSize])

            # Load motion batch to device
            motionBatch = torch.from_numpy(motionBatch).to(device)

            # Mapping the motion batch onto the w-space
            wBatch = self.Gs.mapping(motionBatch, motionBatch)

            # Start the synthesis
            # Disable gradient calculation for generation mode
            # Because we are not in training mode for the GAN, this reduces memory consumption
            with torch.no_grad():
                imageBatch = self.Gs.synthesis(wBatch, **GsSynKWArgs, force_fp32=True).detach().cpu()

            # Save the image in the batch
            for j, image in enumerate(imageBatch):
                imageIndex = i * batchSize + j

                # Clamping RGB Values
                # TODO understand what happens here
                image = (image.permute(1, 2, 0) * 127.5 + 128).clamp(0, 255).to(torch.uint8).squeeze(0)
                array = np.array(image)

                # Turning into image from RGB array
                finalImage = Image.fromarray(array, 'RGB')

                # Storing image with index
                file_name = 'img' + str(imageIndex).zfill(4)
                finalImage.save(os.path.join(path, file_name + '.png'))

            del imageBatch
            del motionBatch
